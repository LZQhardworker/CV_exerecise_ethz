{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data \n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision contains convinience functions for popular datasets\n",
    "ds_train = datasets.MNIST('data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is a 28x28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x198DFEF8780>, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=120x120 at 0x198DBFB2F28>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we index this dataset, we get a single data point: a PIL image and an Integer\n",
    "print(ds_train[0])\n",
    "ds_train[0][0].resize((120,120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the data to something that our Pytorch models will understand\n",
    "for this purpose, we can supply a transform function to the datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "ds_train = datasets.MNIST('data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now a `torch.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization is something you learned about in the lecture. Normalizing with $\\mu=0, \\sigma=1$ corresponds to no normalization. Let's compute the proper normalization constants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get only the images \n",
    "ims_train = ds_train.data\n",
    "ims_train = ims_train.float() / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1307), tensor(0.3081))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################################\n",
    "# TODO: calculate the mean and std of MNIST images\n",
    "# hint: to look for operations on pytorch tensor, refer to the official PyTorch docs \n",
    "# https://pytorch.org/docs/stable/\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "mu =torch.mean(ims_train)\n",
    "std = torch.std(ims_train)\n",
    "mu,std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mu,std),\n",
    "])\n",
    "ds_train = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "ds_test = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "print(ds_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to receive mini-batches, not only single data points.\n",
    "We use PyTorch's DataLoader class. Build a dataloader with a batch size of 64 and 4 workers (number of subprocess that peform the dataloading). Important: you need to shuffle the training data, not the test data.\n",
    "\n",
    "**NOTE**: if you encounter some unexpected errors in data loading, try setting `NUM_WORKERS = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59968\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "#########################################################################\n",
    "# TODO: Build a dataloader for both train and test data.\n",
    "#########################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "            ds_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "            ds_test,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "print(len(ds_train.data) //64 *64)\n",
    "        \n",
    "\n",
    "dl_train = train_dataloader\n",
    "dl_test = test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in Pytorch\n",
    "\n",
    "Ok, the dataloading works. Let's build our model, PyTorch makes this very easy. We will build replicate the model from our last exercises. However, now, we add another variable called `nLayer` that indicates how many linear layers that in your network. Please adapt your code from last exercise accordingly to allow different number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters to be used\n",
    "nInput = 784\n",
    "nOutput = 10\n",
    "nLayer = 2\n",
    "nHidden = 16\n",
    "act_fn = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Implement the __init__ of the MLP class. \n",
    "# insert the activation after every linear layer. Important: the number of \n",
    "# hidden layers should be variable!\n",
    "#########################################################################\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nInput, nOutput, nLayer, nHidden, act_fn):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [] \n",
    "        \n",
    "        ##### implement this part #####\n",
    "        layers.append(nn.Sequential(*[nn.Linear(nInput,nHidden),act_fn]))\n",
    "        for i in range(nLayer-2):\n",
    "            layers.append(nn.Sequential(*[nn.Linear(nHidden,nHidden),act_fn]))\n",
    "        layers.append(nn.Sequential(*[nn.Linear(nHidden,nOutput)]))\n",
    "\n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test if the forward pass works\n",
    "# this should print torch.Size([1, 10])\n",
    "t = torch.randn(1,1,28,28)\n",
    "mlp = MLP(nInput, nOutput, nLayer, nHidden, act_fn)\n",
    "mlp(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the test function for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dl_test, device='cpu'):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dl_test:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(dl_test.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(dl_test.dataset),\n",
    "        100. * correct / len(dl_test.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you only need to implement the training and you are good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Implement the missing part of the training function. As a loss function we want to use cross entropy\n",
    "# It can be called with F.cross_entropy().\n",
    "# Hint: Pass through the model -> Backpropagate gradients -> Take gradient step\n",
    "#########################################################################\n",
    "\n",
    "def train(model, dl_train, optimizer, epoch, log_interval=100, device='cpu'):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(dl_train):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # first we need to zero the gradient, otherwise PyTorch would accumulate them\n",
    "        optimizer.zero_grad()         \n",
    "        \n",
    "        ##### implement this part #####\n",
    "        output=model(data)\n",
    "        loss = F.cross_entropy(\n",
    "            output, target\n",
    "        )\n",
    "        ###############################\n",
    "\n",
    "        # stats\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dl_train.dataset),\n",
    "                100. * batch_idx / len(dl_train), loss.item()))\n",
    "        \n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        loss, correct, len(dl_train.dataset),\n",
    "        100. * correct / len(dl_train.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the setup is almost done. The onoly missing part is the optimizer. We are going to use Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize the mlp, so we can play with parameters right here\n",
    "mlp = MLP(nInput, nOutput, nLayer, nHidden, act_fn)\n",
    "optimizer = optim.Adam(mlp.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295517\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 1.536715\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.868526\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.579430\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.579954\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.635633\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.665823\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.793041\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.734552\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.604768\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.420528\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.277784\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.221361\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.102229\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.290331\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.409204\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.391983\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.257939\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.159212\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.227660\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.180704\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.078789\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.283765\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.287554\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.209267\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.290311\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.365122\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.297305\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.137713\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.580871\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.129881\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.216191\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.367282\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.148328\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.574476\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.170694\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.325811\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.362288\n",
      "\n",
      "Train set: Average loss: 0.2147, Accuracy: 53129/60000 (88.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2650, Accuracy: 9248/10000 (92.480%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.399156\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.528482\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.634543\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.035964\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.345159\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.090006\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.321990\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.217851\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.116866\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.027628\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.138311\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.513041\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.660460\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.333946\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.115222\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.199186\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.299194\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.122894\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.039752\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.369682\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.185229\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.346103\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.044795\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.322992\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.272576\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.214508\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.761522\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.022264\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.048183\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.345623\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.021294\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.371885\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.168193\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.133107\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.116607\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.396126\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.394506\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.053901\n",
      "\n",
      "Train set: Average loss: 0.1631, Accuracy: 55703/60000 (92.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2346, Accuracy: 9342/10000 (93.420%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.347887\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.076756\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.155887\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.272004\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.067875\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.237596\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.383670\n",
      "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.185199\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.044781\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.279689\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.400404\n",
      "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.185859\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.440394\n",
      "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.301460\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.461924\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.329174\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.111846\n",
      "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.055765\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.498401\n",
      "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.301801\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.196236\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.228411\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.330730\n",
      "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.123891\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.101937\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.165657\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.065248\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.250606\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.315409\n",
      "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.197570\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.083098\n",
      "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.022709\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.060651\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.424698\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.418338\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.311813\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.152632\n",
      "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.670487\n",
      "\n",
      "Train set: Average loss: 0.6033, Accuracy: 56364/60000 (93.9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2002, Accuracy: 9431/10000 (94.310%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.259290\n",
      "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.259645\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.358668\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.231709\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.097634\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.361599\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.108535\n",
      "Train Epoch: 4 [11200/60000 (19%)]\tLoss: 0.308110\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.038081\n",
      "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.194268\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.435570\n",
      "Train Epoch: 4 [17600/60000 (29%)]\tLoss: 0.352717\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.071557\n",
      "Train Epoch: 4 [20800/60000 (35%)]\tLoss: 0.066729\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.200106\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.062484\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.076190\n",
      "Train Epoch: 4 [27200/60000 (45%)]\tLoss: 0.070271\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.179345\n",
      "Train Epoch: 4 [30400/60000 (51%)]\tLoss: 0.145264\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.339689\n",
      "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.464491\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.375303\n",
      "Train Epoch: 4 [36800/60000 (61%)]\tLoss: 0.296913\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.127718\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.216783\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.070545\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.223361\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.101333\n",
      "Train Epoch: 4 [46400/60000 (77%)]\tLoss: 0.278582\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.041462\n",
      "Train Epoch: 4 [49600/60000 (83%)]\tLoss: 0.025915\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.447433\n",
      "Train Epoch: 4 [52800/60000 (88%)]\tLoss: 0.248719\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.168481\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.021581\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.055215\n",
      "Train Epoch: 4 [59200/60000 (99%)]\tLoss: 0.018062\n",
      "\n",
      "Train set: Average loss: 0.1080, Accuracy: 56665/60000 (94.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1895, Accuracy: 9479/10000 (94.790%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.162389\n",
      "Train Epoch: 5 [1600/60000 (3%)]\tLoss: 0.020685\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.994516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [4800/60000 (8%)]\tLoss: 0.267569\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.049695\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.179453\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.649060\n",
      "Train Epoch: 5 [11200/60000 (19%)]\tLoss: 0.025393\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.168183\n",
      "Train Epoch: 5 [14400/60000 (24%)]\tLoss: 0.320353\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.461091\n",
      "Train Epoch: 5 [17600/60000 (29%)]\tLoss: 0.270874\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.447881\n",
      "Train Epoch: 5 [20800/60000 (35%)]\tLoss: 0.300256\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.049283\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.005724\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.064714\n",
      "Train Epoch: 5 [27200/60000 (45%)]\tLoss: 0.098619\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.033701\n",
      "Train Epoch: 5 [30400/60000 (51%)]\tLoss: 0.364411\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.061753\n",
      "Train Epoch: 5 [33600/60000 (56%)]\tLoss: 0.033832\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.164811\n",
      "Train Epoch: 5 [36800/60000 (61%)]\tLoss: 0.030777\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.509759\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.041141\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.039657\n",
      "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.059153\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.024916\n",
      "Train Epoch: 5 [46400/60000 (77%)]\tLoss: 0.048062\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.119572\n",
      "Train Epoch: 5 [49600/60000 (83%)]\tLoss: 0.275061\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.250850\n",
      "Train Epoch: 5 [52800/60000 (88%)]\tLoss: 0.448244\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.677059\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.048597\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.249922\n",
      "Train Epoch: 5 [59200/60000 (99%)]\tLoss: 0.042669\n",
      "\n",
      "Train set: Average loss: 0.1935, Accuracy: 56854/60000 (94.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1938, Accuracy: 9479/10000 (94.790%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.163301\n",
      "Train Epoch: 6 [1600/60000 (3%)]\tLoss: 0.028655\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.120286\n",
      "Train Epoch: 6 [4800/60000 (8%)]\tLoss: 0.241591\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.045140\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.029374\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.188861\n",
      "Train Epoch: 6 [11200/60000 (19%)]\tLoss: 0.189259\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.021452\n",
      "Train Epoch: 6 [14400/60000 (24%)]\tLoss: 0.043721\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.264034\n",
      "Train Epoch: 6 [17600/60000 (29%)]\tLoss: 0.020034\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.047779\n",
      "Train Epoch: 6 [20800/60000 (35%)]\tLoss: 0.190086\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.126209\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.041668\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.380165\n",
      "Train Epoch: 6 [27200/60000 (45%)]\tLoss: 0.202121\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.073226\n",
      "Train Epoch: 6 [30400/60000 (51%)]\tLoss: 0.128397\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.015473\n",
      "Train Epoch: 6 [33600/60000 (56%)]\tLoss: 0.114937\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.568399\n",
      "Train Epoch: 6 [36800/60000 (61%)]\tLoss: 0.130218\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.451540\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.064165\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.741942\n",
      "Train Epoch: 6 [43200/60000 (72%)]\tLoss: 0.161117\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.115563\n",
      "Train Epoch: 6 [46400/60000 (77%)]\tLoss: 0.165927\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.274368\n",
      "Train Epoch: 6 [49600/60000 (83%)]\tLoss: 0.045921\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.021778\n",
      "Train Epoch: 6 [52800/60000 (88%)]\tLoss: 0.137193\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.176453\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.122413\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.026519\n",
      "Train Epoch: 6 [59200/60000 (99%)]\tLoss: 0.110800\n",
      "\n",
      "Train set: Average loss: 0.2455, Accuracy: 57006/60000 (95.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1856, Accuracy: 9471/10000 (94.710%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.516882\n",
      "Train Epoch: 7 [1600/60000 (3%)]\tLoss: 0.123598\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.212815\n",
      "Train Epoch: 7 [4800/60000 (8%)]\tLoss: 0.148672\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.039819\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.023221\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.481314\n",
      "Train Epoch: 7 [11200/60000 (19%)]\tLoss: 0.224954\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.098660\n",
      "Train Epoch: 7 [14400/60000 (24%)]\tLoss: 0.145416\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.240778\n",
      "Train Epoch: 7 [17600/60000 (29%)]\tLoss: 0.013631\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.197236\n",
      "Train Epoch: 7 [20800/60000 (35%)]\tLoss: 0.034719\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.393315\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.535242\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.012668\n",
      "Train Epoch: 7 [27200/60000 (45%)]\tLoss: 0.896162\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.254143\n",
      "Train Epoch: 7 [30400/60000 (51%)]\tLoss: 0.334695\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.072984\n",
      "Train Epoch: 7 [33600/60000 (56%)]\tLoss: 0.099394\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.035300\n",
      "Train Epoch: 7 [36800/60000 (61%)]\tLoss: 0.115519\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.043094\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.034108\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.051134\n",
      "Train Epoch: 7 [43200/60000 (72%)]\tLoss: 0.137842\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.230653\n",
      "Train Epoch: 7 [46400/60000 (77%)]\tLoss: 0.443872\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.475217\n",
      "Train Epoch: 7 [49600/60000 (83%)]\tLoss: 0.140735\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.085699\n",
      "Train Epoch: 7 [52800/60000 (88%)]\tLoss: 0.223902\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.023877\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.018411\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.609943\n",
      "Train Epoch: 7 [59200/60000 (99%)]\tLoss: 0.172089\n",
      "\n",
      "Train set: Average loss: 0.0749, Accuracy: 57134/60000 (95.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1764, Accuracy: 9504/10000 (95.040%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.107346\n",
      "Train Epoch: 8 [1600/60000 (3%)]\tLoss: 0.012087\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.426174\n",
      "Train Epoch: 8 [4800/60000 (8%)]\tLoss: 0.048183\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.189843\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.059200\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.038353\n",
      "Train Epoch: 8 [11200/60000 (19%)]\tLoss: 0.282794\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.125244\n",
      "Train Epoch: 8 [14400/60000 (24%)]\tLoss: 0.677602\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.148009\n",
      "Train Epoch: 8 [17600/60000 (29%)]\tLoss: 0.177288\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.118542\n",
      "Train Epoch: 8 [20800/60000 (35%)]\tLoss: 0.248487\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.139242\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.151255\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.093319\n",
      "Train Epoch: 8 [27200/60000 (45%)]\tLoss: 0.382605\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.047675\n",
      "Train Epoch: 8 [30400/60000 (51%)]\tLoss: 0.049222\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.076628\n",
      "Train Epoch: 8 [33600/60000 (56%)]\tLoss: 0.267034\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.035178\n",
      "Train Epoch: 8 [36800/60000 (61%)]\tLoss: 0.225838\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.103117\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.126168\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.178874\n",
      "Train Epoch: 8 [43200/60000 (72%)]\tLoss: 0.034521\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.100951\n",
      "Train Epoch: 8 [46400/60000 (77%)]\tLoss: 0.064289\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.086589\n",
      "Train Epoch: 8 [49600/60000 (83%)]\tLoss: 0.309713\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.284106\n",
      "Train Epoch: 8 [52800/60000 (88%)]\tLoss: 0.235313\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.092467\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.059218\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.042026\n",
      "Train Epoch: 8 [59200/60000 (99%)]\tLoss: 0.098658\n",
      "\n",
      "Train set: Average loss: 0.7703, Accuracy: 57250/60000 (95.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1735, Accuracy: 9500/10000 (95.000%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.142366\n",
      "Train Epoch: 9 [1600/60000 (3%)]\tLoss: 0.027729\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.082086\n",
      "Train Epoch: 9 [4800/60000 (8%)]\tLoss: 0.079676\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.008363\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.018268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.061317\n",
      "Train Epoch: 9 [11200/60000 (19%)]\tLoss: 0.046490\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.080913\n",
      "Train Epoch: 9 [14400/60000 (24%)]\tLoss: 0.735750\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.053910\n",
      "Train Epoch: 9 [17600/60000 (29%)]\tLoss: 0.650012\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.003693\n",
      "Train Epoch: 9 [20800/60000 (35%)]\tLoss: 0.035162\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.049283\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.206800\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.284731\n",
      "Train Epoch: 9 [27200/60000 (45%)]\tLoss: 0.182686\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.305997\n",
      "Train Epoch: 9 [30400/60000 (51%)]\tLoss: 0.116212\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.034315\n",
      "Train Epoch: 9 [33600/60000 (56%)]\tLoss: 0.028327\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.057583\n",
      "Train Epoch: 9 [36800/60000 (61%)]\tLoss: 0.047577\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.529989\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.008900\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.133829\n",
      "Train Epoch: 9 [43200/60000 (72%)]\tLoss: 0.288235\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.027960\n",
      "Train Epoch: 9 [46400/60000 (77%)]\tLoss: 0.013067\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.221323\n",
      "Train Epoch: 9 [49600/60000 (83%)]\tLoss: 0.198047\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.115094\n",
      "Train Epoch: 9 [52800/60000 (88%)]\tLoss: 0.321501\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.014826\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.017722\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.073159\n",
      "Train Epoch: 9 [59200/60000 (99%)]\tLoss: 0.449139\n",
      "\n",
      "Train set: Average loss: 0.0164, Accuracy: 57298/60000 (95.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1718, Accuracy: 9528/10000 (95.280%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.018971\n",
      "Train Epoch: 10 [1600/60000 (3%)]\tLoss: 0.042830\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.232122\n",
      "Train Epoch: 10 [4800/60000 (8%)]\tLoss: 0.067608\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.054731\n",
      "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 0.264551\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.205871\n",
      "Train Epoch: 10 [11200/60000 (19%)]\tLoss: 0.049908\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.174900\n",
      "Train Epoch: 10 [14400/60000 (24%)]\tLoss: 0.203401\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.068767\n",
      "Train Epoch: 10 [17600/60000 (29%)]\tLoss: 0.288187\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.035631\n",
      "Train Epoch: 10 [20800/60000 (35%)]\tLoss: 0.413662\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.270894\n",
      "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 0.174588\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.062078\n",
      "Train Epoch: 10 [27200/60000 (45%)]\tLoss: 0.354369\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.086393\n",
      "Train Epoch: 10 [30400/60000 (51%)]\tLoss: 0.062594\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.510807\n",
      "Train Epoch: 10 [33600/60000 (56%)]\tLoss: 0.127265\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.048104\n",
      "Train Epoch: 10 [36800/60000 (61%)]\tLoss: 0.040479\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.016096\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.027854\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.112035\n",
      "Train Epoch: 10 [43200/60000 (72%)]\tLoss: 0.494399\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.034973\n",
      "Train Epoch: 10 [46400/60000 (77%)]\tLoss: 0.047987\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.338743\n",
      "Train Epoch: 10 [49600/60000 (83%)]\tLoss: 0.048686\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.141746\n",
      "Train Epoch: 10 [52800/60000 (88%)]\tLoss: 0.031988\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.120710\n",
      "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 0.048117\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.019321\n",
      "Train Epoch: 10 [59200/60000 (99%)]\tLoss: 0.723426\n",
      "\n",
      "Train set: Average loss: 0.0694, Accuracy: 57365/60000 (95.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1726, Accuracy: 9515/10000 (95.150%)\n",
      "\n",
      "Training is finished.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(mlp, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(mlp, dl_test)\n",
    "\n",
    "print ('Training is finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you should see test accuracies of > **94%** - By they way, here we report test accuracy, the last exercises reported test error. Accuracy is simply (1 - error). Both metrics are commonly reported, there is no clear preference in literature for one or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you do some parameter tuning to boost the test accuracy to > **97%**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.281527\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.615826\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.241675\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.165483\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.918883\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.202891\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.074245\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.232649\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.392912\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.068562\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.158312\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.042387\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.081032\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.314340\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.458958\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.007648\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.286067\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.471470\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.105697\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.525106\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.067198\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.099568\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.203684\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.291427\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.151115\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.167753\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.024868\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.675564\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.082846\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.218408\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.688512\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.106236\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.033639\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.015554\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.183215\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.039936\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.005163\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.017065\n",
      "\n",
      "Train set: Average loss: 0.0201, Accuracy: 55513/60000 (92.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1402, Accuracy: 9563/10000 (95.630%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.016487\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.053335\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.027832\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.085360\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.182946\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.208512\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.013760\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.006632\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.149960\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.129822\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.068673\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.140641\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.019662\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.025683\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.139209\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.009781\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.113859\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.063989\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.069689\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.260037\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.996387\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.004817\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.159105\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.283794\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.212154\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.204695\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.330283\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.495555\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.057681\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.092126\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.001445\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.046146\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.034680\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.018264\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.051065\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.415347\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.150716\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.007469\n",
      "\n",
      "Train set: Average loss: 0.0108, Accuracy: 57985/60000 (96.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1301, Accuracy: 9600/10000 (96.000%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.211519\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.023517\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.084980\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.292324\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.002117\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.002487\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.003053\n",
      "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.020040\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.007994\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.048669\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.165606\n",
      "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.035884\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.425830\n",
      "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.011220\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.002092\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.127694\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.112121\n",
      "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.027235\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.218301\n",
      "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.059234\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.018917\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.065202\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.008956\n",
      "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.114863\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.091997\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.012248\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.070799\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.186333\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.151540\n",
      "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.030416\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.117764\n",
      "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.104753\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.152921\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.016765\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.000257\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.069527\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.108782\n",
      "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.036261\n",
      "\n",
      "Train set: Average loss: 0.0012, Accuracy: 58530/60000 (97.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0998, Accuracy: 9717/10000 (97.170%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.000998\n",
      "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.000914\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.000078\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.008177\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.000417\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.024676\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.217229\n",
      "Train Epoch: 4 [11200/60000 (19%)]\tLoss: 0.004367\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.027053\n",
      "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.002123\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.422463\n",
      "Train Epoch: 4 [17600/60000 (29%)]\tLoss: 0.001441\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.179535\n",
      "Train Epoch: 4 [20800/60000 (35%)]\tLoss: 0.002809\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.060088\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.038647\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.099771\n",
      "Train Epoch: 4 [27200/60000 (45%)]\tLoss: 0.004300\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.176793\n",
      "Train Epoch: 4 [30400/60000 (51%)]\tLoss: 0.020355\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.258327\n",
      "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.010828\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.040953\n",
      "Train Epoch: 4 [36800/60000 (61%)]\tLoss: 0.002354\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.003999\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.063786\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.022420\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.069585\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.004271\n",
      "Train Epoch: 4 [46400/60000 (77%)]\tLoss: 0.099289\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.059413\n",
      "Train Epoch: 4 [49600/60000 (83%)]\tLoss: 0.097051\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.022339\n",
      "Train Epoch: 4 [52800/60000 (88%)]\tLoss: 0.014256\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.121514\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.002703\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.011332\n",
      "Train Epoch: 4 [59200/60000 (99%)]\tLoss: 0.026265\n",
      "\n",
      "Train set: Average loss: 0.0641, Accuracy: 58824/60000 (98.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0896, Accuracy: 9751/10000 (97.510%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.012745\n",
      "Train Epoch: 5 [1600/60000 (3%)]\tLoss: 0.150915\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.102812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [4800/60000 (8%)]\tLoss: 0.016625\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001028\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.004937\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.015158\n",
      "Train Epoch: 5 [11200/60000 (19%)]\tLoss: 0.001531\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.002074\n",
      "Train Epoch: 5 [14400/60000 (24%)]\tLoss: 0.002970\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.000245\n",
      "Train Epoch: 5 [17600/60000 (29%)]\tLoss: 0.003295\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.094197\n",
      "Train Epoch: 5 [20800/60000 (35%)]\tLoss: 0.225318\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.064536\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.004495\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.298253\n",
      "Train Epoch: 5 [27200/60000 (45%)]\tLoss: 0.001181\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.033662\n",
      "Train Epoch: 5 [30400/60000 (51%)]\tLoss: 0.000408\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.019245\n",
      "Train Epoch: 5 [33600/60000 (56%)]\tLoss: 0.089812\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.000841\n",
      "Train Epoch: 5 [36800/60000 (61%)]\tLoss: 0.128430\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.001797\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.158012\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.000760\n",
      "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.000451\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.021455\n",
      "Train Epoch: 5 [46400/60000 (77%)]\tLoss: 0.068464\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.017656\n",
      "Train Epoch: 5 [49600/60000 (83%)]\tLoss: 0.065631\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.012511\n",
      "Train Epoch: 5 [52800/60000 (88%)]\tLoss: 0.000789\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.104959\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.010073\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.034846\n",
      "Train Epoch: 5 [59200/60000 (99%)]\tLoss: 0.365224\n",
      "\n",
      "Train set: Average loss: 0.0661, Accuracy: 59060/60000 (98.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0852, Accuracy: 9779/10000 (97.790%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.009971\n",
      "Train Epoch: 6 [1600/60000 (3%)]\tLoss: 0.000835\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.004290\n",
      "Train Epoch: 6 [4800/60000 (8%)]\tLoss: 0.001610\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.010515\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.087613\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.007518\n",
      "Train Epoch: 6 [11200/60000 (19%)]\tLoss: 0.002960\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.006550\n",
      "Train Epoch: 6 [14400/60000 (24%)]\tLoss: 0.005536\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.012827\n",
      "Train Epoch: 6 [17600/60000 (29%)]\tLoss: 0.491935\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.003618\n",
      "Train Epoch: 6 [20800/60000 (35%)]\tLoss: 0.005546\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.001315\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.001313\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.045269\n",
      "Train Epoch: 6 [27200/60000 (45%)]\tLoss: 0.002993\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.000078\n",
      "Train Epoch: 6 [30400/60000 (51%)]\tLoss: 0.000651\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.012734\n",
      "Train Epoch: 6 [33600/60000 (56%)]\tLoss: 0.000539\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.002773\n",
      "Train Epoch: 6 [36800/60000 (61%)]\tLoss: 0.000289\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.001369\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.231374\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.001841\n",
      "Train Epoch: 6 [43200/60000 (72%)]\tLoss: 0.009046\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.001257\n",
      "Train Epoch: 6 [46400/60000 (77%)]\tLoss: 0.200029\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.027650\n",
      "Train Epoch: 6 [49600/60000 (83%)]\tLoss: 0.006572\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.003160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-2be78bf01a19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-28668df23713>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dl_train, optimizer, epoch, log_interval, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m# unserialize the data after having released the lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\multiprocessing\\reductions.py\u001b[0m in \u001b[0;36mrebuild_tensor\u001b[1;34m(cls, storage, metadata)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrebuild_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mstorage_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# we have to pass requires_grad into constructor, rather than set it as an\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36m_rebuild_tensor\u001b[1;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_rebuild_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;31m# first construct a tensor with the correct dtype/device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#TODO: modify the parameters below to see which setting that you can get to 97%\n",
    "#########################################################################\n",
    "nLayer = 4\n",
    "nHidden =128\n",
    "act_fn = nn.ReLU()\n",
    "\n",
    "# reinitialize the mlp, so we can play with parameters right here\n",
    "mlp = MLP(nInput, nOutput, nLayer, nHidden, act_fn)\n",
    "optimizer = optim.Adam(mlp.parameters())\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(mlp, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(mlp, dl_test)\n",
    "\n",
    "print ('Training is finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you move on to the next exercise, you can further play with the other parameters (learning rate, epochs, a different optimizer, etc.) to get a feeling what can improve or hamper performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "Alright, we matched our prior performance. Let's surpass it! You will soon see the power of CNN by building a small one yourself. The structure should be as follows\n",
    "\n",
    "| CNN Architecture                             \t|\n",
    "|----------------------------------------------\t|\n",
    "| Conv: $C_{in}=1, C_{out}=32, K=3, S=1, P=0$  \t|\n",
    "| ReLU                                         \t|\n",
    "| Conv: $C_{in}=32, C_{out}=64, K=3, S=1, P=0$ \t|\n",
    "| ReLU                                         \t|\n",
    "| MaxPool2d: $K=2, S=2, P=0$                   \t|\n",
    "| Dropout: $p=0.25$                            \t|\n",
    "| Linear: $C_{in}=9216, C_{out}=128$           \t|\n",
    "| ReLU                                         \t|\n",
    "| Dropout: $p=0.5$                             \t|\n",
    "| Linear: $C_{in}=128, C_{out}=10$             \t|\n",
    "\n",
    "The layers you will need are: \n",
    "\n",
    "`nn.Conv2d,  nn.Linear,  nn.Dropout, nn.MaxPool2d, nn.Flatten`\n",
    "\n",
    "For layers without parameters you can alternatively use function in the forward pass:  \n",
    "\n",
    "`F.max_pool2d, torch.flatten`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Implement the __init__ and forward method of the CNN class. \n",
    "# Hint: do not forget to flatten the appropriate dimension after the convolutional blocks. \n",
    "# A linear layers expect input of the size (B, H) with batch size B and feature size H\n",
    "#########################################################################\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        model=[]\n",
    "        model.extend([nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),nn.ReLU()])\n",
    "        model.extend([nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),nn.ReLU()])\n",
    "        model.extend([nn.MaxPool2d(kernel_size=(2,2))])\n",
    "        model.extend([nn.Dropout(0.25)])\n",
    "        self.model=nn.Sequential(*model)\n",
    "        linear=[]\n",
    "        linear+=[nn.Linear(9216,128),nn.ReLU(),nn.Linear(128,10)]\n",
    "        self.linear=nn.Sequential(*linear)\n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test if the forward pass works\n",
    "# this should print torch.Size([1, 10])\n",
    "t = torch.randn(1,1,28,28)\n",
    "cnn = CNN()\n",
    "cnn(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293592\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.251392\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.174839\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.298850\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.465581\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.071741\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.020922\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.046413\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.048794\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.007504\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.121716\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.237033\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.189350\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.128862\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.067797\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.303096\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.203418\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.007296\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.095829\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.305966\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.017925\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.004891\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.001669\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.015437\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.034085\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.006619\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.042599\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.000986\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.012430\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.032537\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.042044\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.083634\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.196159\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.004446\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.271596\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.010764\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.041692\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.005881\n",
      "\n",
      "Train set: Average loss: 0.0029, Accuracy: 57827/60000 (96.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0481, Accuracy: 9848/10000 (98.480%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.008939\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.002241\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.000947\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.000502\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.000099\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.260316\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.000189\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.015456\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.000501\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.068688\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.000395\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.155086\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001912\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.005352\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.046268\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.041628\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.020499\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.101931\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.075385\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.021871\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.348252\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.001418\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.000112\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.000731\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.215342\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.496534\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.002784\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.009551\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.014647\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.014000\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.004848\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.001557\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.007166\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.162174\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.001708\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.001850\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.000259\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.008807\n",
      "\n",
      "Train set: Average loss: 0.0005, Accuracy: 59260/60000 (98.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0363, Accuracy: 9873/10000 (98.730%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.017445\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.000265\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.004040\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.000038\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.092373\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.016869\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.003264\n",
      "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.000317\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.000095\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.029499\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.164625\n",
      "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.035460\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003364\n",
      "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.002130\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.000084\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.000519\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.000304\n",
      "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.001065\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.001023\n",
      "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.103297\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.000992\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.000710\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.000674\n",
      "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.001376\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001161\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.000367\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.114824\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.001408\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.003656\n",
      "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.094498\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.000268\n",
      "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.050801\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.000604\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.185771\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.004344\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.000340\n",
      "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.000025\n",
      "\n",
      "Train set: Average loss: 0.0215, Accuracy: 59513/60000 (99.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0297, Accuracy: 9907/10000 (99.070%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.015453\n",
      "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.045187\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.029923\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.000174\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.000470\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-83c2d18307df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-28668df23713>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dl_train, optimizer, epoch, log_interval, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will probably take a bit longer to train, as a convolutional network is not very efficient on a CPU. The current settings should get you around **99%** accuracy. Nice! \n",
    "Again, you should try different hyperparameters and see how far you can push the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question\n",
    "\n",
    "If your model weight is randomly initalized, and no training is done as above. What accuracy do you think the model will get for a 10-class classification task in theory?\n",
    "\n",
    "**Your answer**: \n",
    "10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CIFAR10\n",
    "\n",
    "Now we are going to move to something more challenging - CIFAR10. We can reuse most of the code above. Thankfully, CIFAR is also a popular dataset, so we can again make use of a PyTorch convience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ds_train = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "ds_test = datasets.CIFAR10(root='./data', train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is not normalized yet, so we need to calculate the normalization constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims_train = torch.tensor(ds_train.data)\n",
    "ims_train = ims_train.float() / 255.\n",
    "ims_test = torch.tensor(ds_test.data)\n",
    "ims_test = ims_test.float() / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 32, 32, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ims_train.std((0,1,2))\n",
    "ims_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3277, 0.3277, 0.3277]), tensor([0.2470, 0.2435, 0.2616]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################################\n",
    "# TODO: calculate the mean and std of CIFAR\n",
    "# hint: We want the mean and std of the channel dimension, these should\n",
    "# be 3 dimensional\n",
    "#########################################################################\n",
    "mu = ims_train.mean((0,1,2))\n",
    "std = ims_train.std((0,1,2))\n",
    "mu,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3277, 0.3277, 0.3277])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(ims_train, dim=(0,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CIFAR we want to make use of data augmentation to improve generalization. You will find all data augmentations data are included in torchvision here:\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4 # if you encounter some unexpected errors in data loading, try setting `NUM_WORKERS = 0`\n",
    "#########################################################################\n",
    "# TODO: Implement the proper transforms for the training and test dataloaders. \n",
    "# Then build train and test dataloaders with batch size 128 and 4 workers\n",
    "#\n",
    "# Train: \n",
    "# - Apply a random crop with size 32 on a padded version of the image with P=4\n",
    "# - Flip the image horizontally with a probability of 40 %\n",
    "# - Transform to a Tensor\n",
    "# - Normalize with the constants calculated above\n",
    "# Test: \n",
    "# - Transform to a Tensor\n",
    "# - Normalize with the constants calculated above\n",
    "#########################################################################\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop((32,32),4),\n",
    "    transforms.RandomHorizontalFlip(0.4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mu,std),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mu,std),\n",
    "])\n",
    "\n",
    "ds_train = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "ds_test = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "            ds_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS)\n",
    "\n",
    "dl_test = torch.utils.data.DataLoader(\n",
    "            ds_test,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the  optimizer, this time we use SGD. The scheduler adapts the learning rate during traing (you can ignore it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        model=[]\n",
    "        model.extend([nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0),nn.ReLU()])\n",
    "        model.extend([nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),nn.ReLU()])\n",
    "        model.extend([nn.MaxPool2d(kernel_size=(2,2))])\n",
    "        model.extend([nn.Dropout(0.25)])\n",
    "        self.model=nn.Sequential(*model)\n",
    "        linear=[]\n",
    "        linear+=[nn.Linear(12544,128),nn.ReLU(),nn.Linear(128,10)]\n",
    "        self.linear=nn.Sequential(*linear)\n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.linear(x)\n",
    "        return x\n",
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.313746\n",
      "Train Epoch: 1 [1600/50000 (3%)]\tLoss: 2.357586\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.342223\n",
      "Train Epoch: 1 [4800/50000 (10%)]\tLoss: 2.282389\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.286858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-ba9429f4ef5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-28668df23713>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dl_train, optimizer, epoch, log_interval, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# get the index of the max log-probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will not work. You should see the following error message\n",
    "\n",
    "```\n",
    "Given groups=1, weight of size [32, 1, 3, 3], expected input[128, 3, 32, 32] to have 1 channels, but got 3 channels instead\n",
    "```\n",
    "\n",
    "This error is telling us that something is not right in the definition of our model. Copy the CNN class from above and make changes, so the training works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Adapt the definition from the CNN class above to work on CIFAR.\n",
    "# You can copy and run the following prompt for evaluation:\n",
    "# CNN()(torch.randn(1,3,32,32)).shape\n",
    "# It should print 'torch.Size([1, 10])'\n",
    "# Hint: You need to change 2 things. \n",
    "#########################################################################\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        model=[]\n",
    "        model.extend([nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0),nn.ReLU()])\n",
    "        model.extend([nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),nn.ReLU()])\n",
    "        model.extend([nn.MaxPool2d(kernel_size=(2,2))])\n",
    "        model.extend([nn.Dropout(0.25)])\n",
    "        self.model=nn.Sequential(*model)\n",
    "        linear=[]\n",
    "        linear+=[nn.Linear(12544,128),nn.ReLU(),nn.Linear(128,10)]\n",
    "        self.linear=nn.Sequential(*linear)\n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.280474\n",
      "Train Epoch: 1 [1600/50000 (3%)]\tLoss: 2.327960\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.353579\n",
      "Train Epoch: 1 [4800/50000 (10%)]\tLoss: 2.277199\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.297877\n",
      "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2.339089\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.271260\n",
      "Train Epoch: 1 [11200/50000 (22%)]\tLoss: 2.246748\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.325574\n",
      "Train Epoch: 1 [14400/50000 (29%)]\tLoss: 2.344105\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.322162\n",
      "Train Epoch: 1 [17600/50000 (35%)]\tLoss: 2.352163\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.309003\n",
      "Train Epoch: 1 [20800/50000 (42%)]\tLoss: 2.303857\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 2.267087\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 2.352301\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.333178\n",
      "Train Epoch: 1 [27200/50000 (54%)]\tLoss: 2.288054\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 2.264236\n",
      "Train Epoch: 1 [30400/50000 (61%)]\tLoss: 2.342430\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.325562\n",
      "Train Epoch: 1 [33600/50000 (67%)]\tLoss: 2.318676\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 2.289992\n",
      "Train Epoch: 1 [36800/50000 (74%)]\tLoss: 2.316827\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.297196\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 2.319240\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 2.264592\n",
      "Train Epoch: 1 [43200/50000 (86%)]\tLoss: 2.310587\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.316601\n",
      "Train Epoch: 1 [46400/50000 (93%)]\tLoss: 2.298282\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 2.286201\n",
      "Train Epoch: 1 [49600/50000 (99%)]\tLoss: 2.294615\n",
      "\n",
      "Train set: Average loss: 2.3248, Accuracy: 5001/50000 (10.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3174, Accuracy: 1000/10000 (10.000%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.302275\n",
      "Train Epoch: 2 [1600/50000 (3%)]\tLoss: 2.282402\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 2.317573\n",
      "Train Epoch: 2 [4800/50000 (10%)]\tLoss: 2.298984\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.346757\n",
      "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 2.273117\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 2.320719\n",
      "Train Epoch: 2 [11200/50000 (22%)]\tLoss: 2.290431\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.341905\n",
      "Train Epoch: 2 [14400/50000 (29%)]\tLoss: 2.318110\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 2.252024\n",
      "Train Epoch: 2 [17600/50000 (35%)]\tLoss: 2.337349\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.341059\n",
      "Train Epoch: 2 [20800/50000 (42%)]\tLoss: 2.264813\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 2.342990\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 2.345090\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.342625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-fd04bab4f771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-28668df23713>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dl_train, optimizer, epoch, log_interval, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m##### implement this part #####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         loss = F.cross_entropy(\n\u001b[0;32m     19\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-33c594229714>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\conda\\conda\\envs\\pytorch36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 338\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give 40 - 50 % - and if you are not already on Colab it will give you a stressed out laptop. The performance is a lot better than random, but we can definitely do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have fun with GPUs\n",
    "You can already call it a day until this point because we won't grade the rest of the excecise. You can have more fun with the rest :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you didn't already, move to colab. To use a GPU, follow on the collaboratory menu tabs, \"Runtime\" => \"Change runtime type\" and set it to GPU. Then run the same training loop but now on GPU. \n",
    "\n",
    "It as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "if device == 'cuda': torch.backends.cudnn.benchmark = True # additional speed up\n",
    "\n",
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100, device=device)\n",
    "    test(cnn, dl_test, device=device)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be way faster now. But the true advantage of the GPU is that we can use much bigger models now and still train them in a reasonable amount of time. PyTorch is again very handy. The torchvision library comes with varies state-of-the-art model architectures, some of which you have seen in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = resnet18()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks scary! But the only thing you need to change to make it work on CIFAR is the last layer.\n",
    "Currently the last layer is:\n",
    "```\n",
    "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "```\n",
    "out_features is the number of classes. This models are developed for Imagenet, a dataset with 1000 classes. So this part of the model you need to adapt. Additionally, you need to add a log-softmax layer again, as we us negative log-likelihood as the training criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Adapt the Resnet to work on CIFAR\n",
    "#########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should print 'torch.Size([16, 10])'\n",
    "cnn(torch.randn(16,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "if device == 'cuda': torch.backends.cudnn.benchmark = True # this gives us additional speed up\n",
    "\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100, device=device)\n",
    "    test(cnn, dl_test, device=device)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should get us well above 75%, the best we got was ~ 80%.\n",
    "\n",
    "Now, use different torchvision architectures, different optimizers (Adam is always a good choice), data augmentation techniques, and hyperparameter search to achieve a test accuracy of >90 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d268b61a0efacafa8645774cb6d0204c9f01d7563ef03f7672146d044e8f345c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
